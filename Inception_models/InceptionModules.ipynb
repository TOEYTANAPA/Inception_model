{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "# start_time = time.time()\n",
    "# # main()\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data and preprocessing and miscellaneous functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use mnist_train.csv as our training data and mnist_test.csv as our validation set\n",
    "http://pjreddie.com/projects/mnist-in-csv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 5.92461085319519 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train_set = pd.read_csv('mnist_train.csv',header=None)\n",
    "test_set = pd.read_csv('mnist_test.csv',header=None)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 6.690660238265991 seconds ---\n"
     ]
    }
   ],
   "source": [
    "new_df = pd.DataFrame(columns = ['modules','time_used/sec',])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "train_set = pd.read_csv('mnist_train.csv',header=None)\n",
    "test_set = pd.read_csv('mnist_test.csv',header=None)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "new_df.loc[len(new_df)] = [\"preprocess\",(time.time() - start_time)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "      <th>784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9   ...   775  776  777  778  \\\n",
       "0    5    0    0    0    0    0    0    0    0    0 ...     0    0    0    0   \n",
       "1    0    0    0    0    0    0    0    0    0    0 ...     0    0    0    0   \n",
       "2    4    0    0    0    0    0    0    0    0    0 ...     0    0    0    0   \n",
       "3    1    0    0    0    0    0    0    0    0    0 ...     0    0    0    0   \n",
       "4    9    0    0    0    0    0    0    0    0    0 ...     0    0    0    0   \n",
       "\n",
       "   779  780  781  782  783  784  \n",
       "0    0    0    0    0    0    0  \n",
       "1    0    0    0    0    0    0  \n",
       "2    0    0    0    0    0    0  \n",
       "3    0    0    0    0    0    0  \n",
       "4    0    0    0    0    0    0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_set = test_set[:5]\n",
    "# train_set = train_set[:5]\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.6712021827697754 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#get labels in own array\n",
    "train_lb=np.array(train_set[0])\n",
    "test_lb=np.array(test_set[0])\n",
    "\n",
    "#one hot encode the labels\n",
    "train_lb=(np.arange(10) == train_lb[:,None]).astype(np.float32)\n",
    "test_lb=(np.arange(10) == test_lb[:,None]).astype(np.float32)\n",
    "\n",
    "#drop the labels column from training dataframe\n",
    "trainX=train_set.drop(0,axis=1)\n",
    "testX=test_set.drop(0,axis=1)\n",
    "\n",
    "#put in correct float32 array format\n",
    "trainX=np.array(trainX).astype(np.float32)\n",
    "testX=np.array(testX).astype(np.float32)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "new_df.loc[len(new_df)] = [\"preprocess\",(time.time() - start_time)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.0009992122650146484 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#reformat the data so it's not flat\n",
    "trainX=trainX.reshape(len(trainX),28,28,1)\n",
    "testX = testX.reshape(len(testX),28,28,1)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "new_df.loc[len(new_df)] = [\"reformat the data\",(time.time() - start_time)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.025929927825927734 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#get a validation set and remove it from the train set\n",
    "trainX,valX,train_lb,val_lb=trainX[0:(len(trainX)-500),:,:,:],trainX[(len(trainX)-500):len(trainX),:,:,:],\\\n",
    "                            train_lb[0:(len(trainX)-500),:],train_lb[(len(trainX)-500):len(trainX),:]\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "new_df.loc[len(new_df)] = [\"get a validation\",(time.time() - start_time)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.04089021682739258 seconds ---\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24a83a8f438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#make sure the images are alright\n",
    "plt.imshow(trainX.reshape(len(trainX),28,28)[0],cmap=\"Greys\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.0 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#need to batch the test data because running low on memory\n",
    "class test_batchs:\n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "        self.batch_index = 0\n",
    "    def nextBatch(self,batch_size):\n",
    "        if (batch_size+self.batch_index) > self.data.shape[0]:\n",
    "            print (\"batch sized is messed up\")\n",
    "        batch = self.data[self.batch_index:(self.batch_index+batch_size),:,:,:]\n",
    "        self.batch_index= self.batch_index+batch_size\n",
    "        return batch\n",
    "\n",
    "#set the test batchsize\n",
    "test_batch_size = 100\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.0 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#returns accuracy of model\n",
    "def accuracy(target,predictions):\n",
    "    return(100.0*np.sum(np.argmax(target,1) == np.argmax(predictions,1))/target.shape[0])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use os to get our current working directory so we can save variable\n",
    "file_path = os.getcwd()+'/model.ckpt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Inception modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 48\n",
    "map1 = 32\n",
    "map2 = 64\n",
    "num_fc1 = 700 #1028\n",
    "num_fc2 = 10\n",
    "reduce1x1 = 16\n",
    "dropout=0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    #train data and labels\n",
    "    X = tf.placeholder(tf.float32,shape=(batch_size,28,28,1))\n",
    "    y_ = tf.placeholder(tf.float32,shape=(batch_size,10))\n",
    "    \n",
    "    #validation data\n",
    "    tf_valX = tf.placeholder(tf.float32,shape=(len(valX),28,28,1))\n",
    "    \n",
    "    #test data\n",
    "    tf_testX=tf.placeholder(tf.float32,shape=(test_batch_size,28,28,1))\n",
    "    \n",
    "    def createWeight(size,Name):\n",
    "        return tf.Variable(tf.truncated_normal(size, stddev=0.1),\n",
    "                          name=Name)\n",
    "    \n",
    "    def createBias(size,Name):\n",
    "        return tf.Variable(tf.constant(0.1,shape=size),\n",
    "                          name=Name)\n",
    "    \n",
    "    def conv2d_s1(x,W):\n",
    "        return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')\n",
    "    \n",
    "    def max_pool_3x3_s1(x):\n",
    "        return tf.nn.max_pool(x,ksize=[1,3,3,1],\n",
    "                             strides=[1,1,1,1],padding='SAME')\n",
    "    \n",
    "    \n",
    "    #Inception Module1\n",
    "    #\n",
    "    #follows input\n",
    "    W_conv1_1x1_1 = createWeight([1,1,1,map1],'W_conv1_1x1_1')\n",
    "    b_conv1_1x1_1 = createWeight([map1],'b_conv1_1x1_1')\n",
    "    \n",
    "    #follows input\n",
    "    W_conv1_1x1_2 = createWeight([1,1,1,reduce1x1],'W_conv1_1x1_2')\n",
    "    b_conv1_1x1_2 = createWeight([reduce1x1],'b_conv1_1x1_2')\n",
    "    \n",
    "    #follows input\n",
    "    W_conv1_1x1_3 = createWeight([1,1,1,reduce1x1],'W_conv1_1x1_3')\n",
    "    b_conv1_1x1_3 = createWeight([reduce1x1],'b_conv1_1x1_3')\n",
    "    \n",
    "    #follows 1x1_2\n",
    "    W_conv1_3x3 = createWeight([3,3,reduce1x1,map1],'W_conv1_3x3')\n",
    "    b_conv1_3x3 = createWeight([map1],'b_conv1_3x3')\n",
    "    \n",
    "    #follows 1x1_3\n",
    "    W_conv1_5x5 = createWeight([5,5,reduce1x1,map1],'W_conv1_5x5')\n",
    "    b_conv1_5x5 = createBias([map1],'b_conv1_5x5')\n",
    "    \n",
    "    #follows max pooling\n",
    "    W_conv1_1x1_4= createWeight([1,1,1,map1],'W_conv1_1x1_4')\n",
    "    b_conv1_1x1_4= createWeight([map1],'b_conv1_1x1_4')\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Inception Module2\n",
    "    #\n",
    "    #follows inception1\n",
    "    W_conv2_1x1_1 = createWeight([1,1,4*map1,map2],'W_conv2_1x1_1')\n",
    "    b_conv2_1x1_1 = createWeight([map2],'b_conv2_1x1_1')\n",
    "    \n",
    "    #follows inception1\n",
    "    W_conv2_1x1_2 = createWeight([1,1,4*map1,reduce1x1],'W_conv2_1x1_2')\n",
    "    b_conv2_1x1_2 = createWeight([reduce1x1],'b_conv2_1x1_2')\n",
    "    \n",
    "    #follows inception1\n",
    "    W_conv2_1x1_3 = createWeight([1,1,4*map1,reduce1x1],'W_conv2_1x1_3')\n",
    "    b_conv2_1x1_3 = createWeight([reduce1x1],'b_conv2_1x1_3')\n",
    "    \n",
    "    #follows 1x1_2\n",
    "    W_conv2_3x3 = createWeight([3,3,reduce1x1,map2],'W_conv2_3x3')\n",
    "    b_conv2_3x3 = createWeight([map2],'b_conv2_3x3')\n",
    "    \n",
    "    #follows 1x1_3\n",
    "    W_conv2_5x5 = createWeight([5,5,reduce1x1,map2],'W_conv2_5x5')\n",
    "    b_conv2_5x5 = createBias([map2],'b_conv2_5x5')\n",
    "    \n",
    "    #follows max pooling\n",
    "    W_conv2_1x1_4= createWeight([1,1,4*map1,map2],'W_conv2_1x1_4')\n",
    "    b_conv2_1x1_4= createWeight([map2],'b_conv2_1x1_4')\n",
    "    \n",
    "    \n",
    "\n",
    "    #Fully connected layers\n",
    "    #since padding is same, the feature map with there will be 4 28*28*map2\n",
    "    W_fc1 = createWeight([28*28*(4*map2),num_fc1],'W_fc1')\n",
    "    b_fc1 = createBias([num_fc1],'b_fc1')\n",
    "    \n",
    "    W_fc2 = createWeight([num_fc1,num_fc2],'W_fc2')\n",
    "    b_fc2 = createBias([num_fc2],'b_fc2')\n",
    "\n",
    "    def model(x,train=True):\n",
    "        #Inception Module 1\n",
    "        \n",
    "        start_time = time.time()\n",
    "        conv1_1x1_1 = conv2d_s1(x,W_conv1_1x1_1)+b_conv1_1x1_1\n",
    "        new_df.loc[len(new_df)] = ['conv1_1x1_1',(time.time() - start_time)]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        conv1_1x1_2 = tf.nn.relu(conv2d_s1(x,W_conv1_1x1_2)+b_conv1_1x1_2)\n",
    "        new_df.loc[len(new_df)] = ['conv1_1x1_2',(time.time() - start_time)]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        conv1_1x1_3 = tf.nn.relu(conv2d_s1(x,W_conv1_1x1_3)+b_conv1_1x1_3)\n",
    "        new_df.loc[len(new_df)] = ['conv1_1x1_3' ,(time.time() - start_time)]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        conv1_3x3 = conv2d_s1(conv1_1x1_2,W_conv1_3x3)+b_conv1_3x3\n",
    "        new_df.loc[len(new_df)] = ['conv1_3x3' ,(time.time() - start_time)]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        conv1_5x5 = conv2d_s1(conv1_1x1_3,W_conv1_5x5)+b_conv1_5x5\n",
    "        new_df.loc[len(new_df)] = ['conv1_5x5' ,(time.time() - start_time)]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        maxpool1 = max_pool_3x3_s1(x)\n",
    "        new_df.loc[len(new_df)] = ['maxpool1' ,(time.time() - start_time)]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        conv1_1x1_4 = conv2d_s1(maxpool1,W_conv1_1x1_4)+b_conv1_1x1_4\n",
    "        new_df.loc[len(new_df)] = ['conv1_1x1_4' ,(time.time() - start_time)]\n",
    "        \n",
    "        #concatenate all the feature maps and hit them with a relu\n",
    "        inception1 = tf.nn.relu(tf.concat([conv1_1x1_1,conv1_3x3,conv1_5x5,conv1_1x1_4],3))\n",
    "\n",
    "        \n",
    "        #Inception Module 2\n",
    "        start_time = time.time()\n",
    "        conv2_1x1_1 = conv2d_s1(inception1,W_conv2_1x1_1)+b_conv2_1x1_1\n",
    "        new_df.loc[len(new_df)] = ['conv2_1x1_1' ,(time.time() - start_time)]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        conv2_1x1_2 = tf.nn.relu(conv2d_s1(inception1,W_conv2_1x1_2)+b_conv2_1x1_2)\n",
    "        new_df.loc[len(new_df)] = ['conv2_1x1_2' ,(time.time() - start_time)]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        conv2_1x1_3 = tf.nn.relu(conv2d_s1(inception1,W_conv2_1x1_3)+b_conv2_1x1_3)\n",
    "        new_df.loc[len(new_df)] = ['conv2_1x1_3' ,(time.time() - start_time)]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        conv2_3x3 = conv2d_s1(conv2_1x1_2,W_conv2_3x3)+b_conv2_3x3\n",
    "        new_df.loc[len(new_df)] = ['conv2_3x3' ,(time.time() - start_time)]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        conv2_5x5 = conv2d_s1(conv2_1x1_3,W_conv2_5x5)+b_conv2_5x5\n",
    "        new_df.loc[len(new_df)] = ['conv2_5x5' ,(time.time() - start_time)]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        maxpool2 = max_pool_3x3_s1(inception1)\n",
    "        new_df.loc[len(new_df)] = ['maxpool2' ,(time.time() - start_time)]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        conv2_1x1_4 = conv2d_s1(maxpool2,W_conv2_1x1_4)+b_conv2_1x1_4\n",
    "        new_df.loc[len(new_df)] = ['conv2_1x1_4' ,(time.time() - start_time)]\n",
    "        \n",
    "        #concatenate all the feature maps and hit them with a relu\n",
    "        start_time = time.time()\n",
    "        inception2 = tf.nn.relu(tf.concat([conv2_1x1_1,conv2_3x3,conv2_5x5,conv2_1x1_4],3))\n",
    "        new_df.loc[len(new_df)] = ['inception2' ,(time.time() - start_time)]\n",
    "\n",
    "        #flatten features for fully connected layer\n",
    "        start_time = time.time()\n",
    "        inception2_flat = tf.reshape(inception2,[-1,28*28*4*map2])\n",
    "        new_df.loc[len(new_df)] = ['inception2_flat' ,(time.time() - start_time)]\n",
    "        \n",
    "        #Fully connected layers\n",
    "        if train:\n",
    "            h_fc1 =tf.nn.dropout(tf.nn.relu(tf.matmul(inception2_flat,W_fc1)+b_fc1),dropout)\n",
    "        else:\n",
    "            h_fc1 = tf.nn.relu(tf.matmul(inception2_flat,W_fc1)+b_fc1)\n",
    "\n",
    "        return tf.matmul(h_fc1,W_fc2)+b_fc2\n",
    "    \n",
    "#     tf.nn.softmax_cross_entropy_with_logits(logits = yPredbyNN, labels=Y)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=model(X),labels=y_))\n",
    "    opt = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "    \n",
    "    predictions_val = tf.nn.softmax(model(tf_valX,train=False))\n",
    "    predictions_test = tf.nn.softmax(model(tf_testX,train=False))\n",
    "    \n",
    "    #initialize variable\n",
    "    init = tf.initialize_all_variables()\n",
    "    \n",
    "    #use to save variables so we can pick up later\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized.\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20000\n",
    "sess = tf.Session(graph=graph)\n",
    "\n",
    "#initialize variables\n",
    "sess.run(init)\n",
    "print(\"Model initialized.\")\n",
    "\n",
    "#set use_previous=1 to use file_path model\n",
    "#set use_previous=0 to start model from scratch\n",
    "use_previous = 0\n",
    "\n",
    "#use the previous model or don't and initialize variables\n",
    "if use_previous:\n",
    "    saver.restore(sess,file_path)\n",
    "    print(\"Model restored.\")\n",
    "\n",
    "#training\n",
    "for s in range(num_steps):\n",
    "    start_time = time.time()\n",
    "\n",
    "    offset = (s*batch_size) % (len(trainX)-batch_size)\n",
    "    batch_x,batch_y = trainX[offset:(offset+batch_size),:],train_lb[offset:(offset+batch_size),:]\n",
    "    feed_dict={X : batch_x, y_ : batch_y}\n",
    "    _,loss_value = sess.run([opt,loss],feed_dict=feed_dict)\n",
    "    \n",
    "    print(\"step\",s)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "#     new_entry = {'time_used/sec': 'device1', 'months': 'month1'}\n",
    "#     new_df = pd.concat([new_df, \"step\", axis=0)\n",
    "#     new_df.loc[len(new_df)] = ['step {}'.format(s) ,(time.time() - start_time)]\n",
    "    \n",
    "    \n",
    "    if s%100 == 0:\n",
    "        feed_dict = {tf_valX : valX}\n",
    "        preds=sess.run(predictions_val,feed_dict=feed_dict)\n",
    "        \n",
    "        print (\"step: \"+str(s))\n",
    "        print (\"validation accuracy: \"+str(accuracy(val_lb,preds)))\n",
    "        print (\" \")\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "        \n",
    "    #get test accuracy and save model\n",
    "    if s == (num_steps-1):\n",
    "        #create an array to store the outputs for the test\n",
    "        result = np.array([]).reshape(0,10)\n",
    "\n",
    "        #use the batches class\n",
    "        batch_testX=test_batchs(testX)\n",
    "\n",
    "        for i in range(len(testX)/test_batch_size):\n",
    "            feed_dict = {tf_testX : batch_testX.nextBatch(test_batch_size)}\n",
    "            preds=sess.run(predictions_test, feed_dict=feed_dict)\n",
    "            result=np.concatenate((result,preds),axis=0)\n",
    "        \n",
    "        print (\"test accuracy: \"+str(accuracy(test_lb,result)))\n",
    "        \n",
    "        save_path = saver.save(sess,file_path)\n",
    "        print(\"Model saved.\")\n",
    "sess.close()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modules</th>\n",
       "      <th>time_used/sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>preprocess</td>\n",
       "      <td>6.691655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>preprocess</td>\n",
       "      <td>1.015813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>reformat the data</td>\n",
       "      <td>0.000992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>get a validation</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>preprocess</td>\n",
       "      <td>0.671202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>reformat the data</td>\n",
       "      <td>0.000999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>get a validation</td>\n",
       "      <td>0.025930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>conv1_1x1_1</td>\n",
       "      <td>0.000997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>conv1_1x1_2</td>\n",
       "      <td>0.000998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>conv1_1x1_3</td>\n",
       "      <td>0.001994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>conv1_3x3</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>conv1_5x5</td>\n",
       "      <td>0.000997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>maxpool1</td>\n",
       "      <td>0.000997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>conv1_1x1_4</td>\n",
       "      <td>0.004986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>conv1_1x1_1</td>\n",
       "      <td>0.000988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>conv1_1x1_2</td>\n",
       "      <td>0.001995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>conv1_1x1_3</td>\n",
       "      <td>0.001995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>conv1_3x3</td>\n",
       "      <td>0.000997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>conv1_5x5</td>\n",
       "      <td>0.000997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>maxpool1</td>\n",
       "      <td>0.000997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>conv1_1x1_4</td>\n",
       "      <td>0.001992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>conv1_1x1_1</td>\n",
       "      <td>0.000997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>conv1_1x1_2</td>\n",
       "      <td>0.001994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>conv1_1x1_3</td>\n",
       "      <td>0.002001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>conv1_3x3</td>\n",
       "      <td>0.001994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>conv1_5x5</td>\n",
       "      <td>0.002019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>maxpool1</td>\n",
       "      <td>0.000997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>conv1_1x1_4</td>\n",
       "      <td>0.002993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              modules  time_used/sec\n",
       "0          preprocess       6.691655\n",
       "1          preprocess       1.015813\n",
       "2   reformat the data       0.000992\n",
       "3    get a validation       0.000000\n",
       "4          preprocess       0.671202\n",
       "5   reformat the data       0.000999\n",
       "6    get a validation       0.025930\n",
       "7         conv1_1x1_1       0.000997\n",
       "8         conv1_1x1_2       0.000998\n",
       "9         conv1_1x1_3       0.001994\n",
       "10          conv1_3x3       0.001000\n",
       "11          conv1_5x5       0.000997\n",
       "12           maxpool1       0.000997\n",
       "13        conv1_1x1_4       0.004986\n",
       "14        conv1_1x1_1       0.000988\n",
       "15        conv1_1x1_2       0.001995\n",
       "16        conv1_1x1_3       0.001995\n",
       "17          conv1_3x3       0.000997\n",
       "18          conv1_5x5       0.000997\n",
       "19           maxpool1       0.000997\n",
       "20        conv1_1x1_4       0.001992\n",
       "21        conv1_1x1_1       0.000997\n",
       "22        conv1_1x1_2       0.001994\n",
       "23        conv1_1x1_3       0.002001\n",
       "24          conv1_3x3       0.001994\n",
       "25          conv1_5x5       0.002019\n",
       "26           maxpool1       0.000997\n",
       "27        conv1_1x1_4       0.002993"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
