{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-72dc95f7812d>:22: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "time   0.017441749572753906\n",
      "WARNING:tensorflow:From <ipython-input-2-72dc95f7812d>:133: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Cost:  2.8867478\n",
      "Accuracy:  5.000000074505806 %\n",
      "Cost:  0.12059539\n",
      "Accuracy:  95.99999785423279 %\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "import time\n",
    "# Specify a device to use\n",
    "with tf.device('/device:GPU:3'):\n",
    "#Why mnist tensorflow?\n",
    "#TensorFlow makes it easy for us to download the MNIST dataset and save\n",
    "#it locally. Our data has been split into a training set on which our \n",
    "#network will learn and a test set against which we’ll check how well we’ve done.\n",
    "\n",
    "#The labels are represnted using one-hot encoding which means:\n",
    "#0 is represented by 1 0 0 0 0 0 0 0 0 0\n",
    "#1 is represented by 0 1 0 0 0 0 0 0 0 0\n",
    "#…\n",
    "#9 is represented by 0 0 0 0 0 0 0 0 0 1\n",
    "    mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "    train_images = np.reshape(mnist.train.images, (-1, 28, 28, 1))\n",
    "    train_labels = mnist.train.labels\n",
    "    test_images = np.reshape(mnist.test.images, (-1, 28, 28, 1))\n",
    "    test_labels = mnist.test.labels\n",
    "\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "    \n",
    "# we use tf.Placeholder to create two \"Placeholder\" nodes in out graph.\n",
    "# input which will contain batches of 100 images,each with 784 values.\n",
    "# labels which will contain batches of 100 labels, each with 10 values.\n",
    "        input = tf.placeholder(tf.float32, shape=(None, 28, 28, 1))\n",
    "        labels = tf.placeholder(tf.float32, shape=(None, 10))\n",
    "#We’ll also pad our 28x28 images to be of size 32x32 so the widths and heights divide by two cleanly.\n",
    "        padded_input = tf.image.resize_image_with_crop_or_pad(input, target_height=32, target_width=32)\n",
    "#we use tf.Variable to created two new nodes, layer1_weights and layer1_biases.\n",
    "#These represent parameters that the network will adjust as we show it more and\n",
    "#more example.\n",
    "\n",
    "# layer1_weights completely random.\n",
    "# layer1_biases allzero.\n",
    "# Mind you will learn more, we'll see that thesr aren't the gteatest choice,but\n",
    "# they'll work for now.\n",
    "#[batch_stride, height_stride, width_stride, depth_stride]\n",
    "        stime = time.time()\n",
    "        layer1_weights = tf.get_variable(\"layer1_weights\", [3, 3, 1, 64], initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "        layer1_bias = tf.Variable(tf.zeros([64]))\n",
    "\n",
    "# we convert logits to a set of predictions using tf.nn.softmax. \n",
    "#tf.nn.relu() in layer1 and layer2. Note that it is applied to \n",
    "#the result of the matrix multiplication of the previous layer’s output\n",
    "# with the current layer’s weights.  \n",
    "        layer1_conv = tf.nn.conv2d(padded_input, filter=layer1_weights, strides=[1,1,1,1], padding='SAME')\n",
    "#What is a Relu?\n",
    "#Relu stands for \"Rectified Linear Unit\" and is an activation function.\n",
    "   \n",
    "#An activation function is applied to the output of each layer of a\n",
    "#neural network. It turns out that if we don't include activation functions\n",
    "#it can be mathematically shown that our three layer network is equivalent to a\n",
    "#single layer network.     \n",
    " \n",
    "        layer1_out = tf.nn.relu(layer1_conv + layer1_bias)\n",
    "        totalt1 = time.time()-stime\n",
    "        print('time  ',totalt1)\n",
    "        \n",
    "        layer2_weights = tf.get_variable(\"layer2_weights\", [3, 3, 64, 64], initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "        layer2_bias = tf.Variable(tf.zeros([64]))\n",
    "        layer2_conv = tf.nn.conv2d(layer1_out, filter=layer2_weights, strides=[1,1,1,1], padding='SAME')\n",
    "        layer2_out = tf.nn.relu(layer2_conv + layer2_bias)\n",
    "\n",
    "        pool1 = tf.nn.max_pool(layer2_out, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "\n",
    "        layer3_weights = tf.get_variable(\"layer3_weights\", [3, 3, 64, 128], initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "        layer3_bias = tf.Variable(tf.zeros([128]))\n",
    "        layer3_conv = tf.nn.conv2d(pool1, filter=layer3_weights, strides=[1,1,1,1], padding='SAME')\n",
    "        layer3_out = tf.nn.relu(layer3_conv + layer3_bias)\n",
    "\n",
    "        layer4_weights = tf.get_variable(\"layer4_weights\", [3, 3, 128, 128], initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "        layer4_bias = tf.Variable(tf.zeros([128]))\n",
    "        layer4_conv = tf.nn.conv2d(layer3_out, filter=layer4_weights, strides=[1,1,1,1], padding='SAME')\n",
    "        layer4_out = tf.nn.relu(layer4_conv + layer4_bias)\n",
    "\n",
    "        pool2 = tf.nn.max_pool(layer4_out, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "\n",
    "        layer5_weights = tf.get_variable(\"layer5_weights\", [3, 3, 128, 256], initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "        layer5_bias = tf.Variable(tf.zeros([256]))\n",
    "        layer5_conv = tf.nn.conv2d(pool2, filter=layer5_weights, strides=[1,1,1,1], padding='SAME')\n",
    "        layer5_out = tf.nn.relu(layer5_conv + layer5_bias)\n",
    "\n",
    "        layer6_weights = tf.get_variable(\"layer6_weights\", [3, 3, 256, 256], initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "        layer6_bias = tf.Variable(tf.zeros([256]))\n",
    "        layer6_conv = tf.nn.conv2d(layer5_out, filter=layer6_weights, strides=[1,1,1,1], padding='SAME')\n",
    "        layer6_out = tf.nn.relu(layer6_conv + layer6_bias)\n",
    "\n",
    "        layer7_weights = tf.get_variable(\"layer7_weights\", [3, 3, 256, 256], initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "        layer7_bias = tf.Variable(tf.zeros([256]))\n",
    "        layer7_conv = tf.nn.conv2d(layer6_out, filter=layer7_weights, strides=[1,1,1,1], padding='SAME')\n",
    "        layer7_out = tf.nn.relu(layer7_conv + layer7_bias)\n",
    "\n",
    "        pool3 = tf.nn.max_pool(layer7_out, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "\n",
    "        layer8_weights = tf.get_variable(\"layer8_weights\", [3, 3, 256, 512], initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "        layer8_bias = tf.Variable(tf.zeros([512]))\n",
    "        layer8_conv = tf.nn.conv2d(pool3, filter=layer8_weights, strides=[1,1,1,1], padding='SAME')\n",
    "        layer8_out = tf.nn.relu(layer8_conv + layer8_bias)\n",
    "\n",
    "        layer9_weights = tf.get_variable(\"layer9_weights\", [3, 3, 512, 512], initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "        layer9_bias = tf.Variable(tf.zeros([512]))\n",
    "        layer9_conv = tf.nn.conv2d(layer8_out, filter=layer9_weights, strides=[1,1,1,1], padding='SAME')\n",
    "        layer9_out = tf.nn.relu(layer9_conv + layer9_bias)\n",
    "\n",
    "        layer10_weights = tf.get_variable(\"layer10_weights\", [3, 3, 512, 512], initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "        layer10_bias = tf.Variable(tf.zeros([512]))\n",
    "        layer10_conv = tf.nn.conv2d(layer9_out, filter=layer10_weights, strides=[1,1,1,1], padding='SAME')\n",
    "        layer10_out = tf.nn.relu(layer10_conv + layer10_bias)\n",
    "\n",
    "        pool4 = tf.nn.max_pool(layer10_out, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "\n",
    "        shape = pool4.shape.as_list()\n",
    "        newShape = shape[1] * shape[2] * shape[3]\n",
    "        reshaped_pool4 = tf.reshape(pool4, [-1, newShape])\n",
    "\n",
    "        fc1_weights = tf.get_variable(\"layer11_weights\", [newShape, 4096], initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "        fc1_bias =  tf.Variable(tf.zeros([4096]))\n",
    "        fc1_out = tf.nn.relu(tf.matmul(reshaped_pool4, fc1_weights) + fc1_bias)\n",
    "\n",
    "        fc2_weights = tf.get_variable(\"layer12_weights\", [4096, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        fc2_bias =  tf.Variable(tf.zeros([10]))\n",
    "        logits = tf.matmul(fc1_out, fc2_weights) + fc2_bias\n",
    "\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "\n",
    "        learning_rate = 0.001\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "        #Add a few nodes to calculate accuracy and optionally retrieve predictions\n",
    "        predictions = tf.nn.softmax(logits)\n",
    "        correct_prediction = tf.equal(tf.argmax(labels, 1), tf.argmax(predictions, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        config = tf.ConfigProto(log_device_placement=True)\n",
    "        config.gpu_options.per_process_gpu_memory_fraction = 0.1\n",
    "        config.gpu_options.allow_growth = True #allocate dynamically\n",
    "        \n",
    "        with tf.Session(graph=graph) as session:\n",
    "            tf.global_variables_initializer().run()\n",
    "\n",
    "            num_steps = 1000\n",
    "            batch_size = 100\n",
    "            for step in range(num_steps):\n",
    "                offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "                batch_images = train_images[offset:(offset + batch_size), :]\n",
    "                batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "                feed_dict = {input: batch_images, labels: batch_labels}\n",
    "\n",
    "                _, c, acc = session.run([optimizer, cost, accuracy], feed_dict=feed_dict)\n",
    "\n",
    "                if step % 100 == 0: \n",
    "                    print(\"Cost: \", c)\n",
    "                    print(\"Accuracy: \", acc * 100.0, \"%\")\n",
    "\n",
    "            #Test \n",
    "            num_test_batches = int(len(test_images) / 100)\n",
    "            total_accuracy = 0\n",
    "            total_cost = 0\n",
    "            for step in range(num_test_batches):\n",
    "                offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "                batch_images = test_images[offset:(offset + batch_size)]\n",
    "                batch_labels = test_labels[offset:(offset + batch_size)]\n",
    "                feed_dict = {input: batch_images, labels: batch_labels}\n",
    "\n",
    "                _, c, acc = session.run([optimizer, cost, accuracy], feed_dict=feed_dict)\n",
    "                total_cost = total_cost + c\n",
    "                total_accuracy = total_accuracy + acc\n",
    "\n",
    "            print(\"Test Cost: \", total_cost / num_test_batches)\n",
    "            print(\"Test accuracy: \", total_accuracy * 100.0 / num_test_batches, \"%\")\n",
    "\n",
    "            session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
